# 自然语言处理导论·2019春·第一次作业报告

## 任务、数据集情况和评价标准

本次作业的任务是检索式对话系统，即根据对话历史 (context)，从 10个候选回复 (reply) 中挑出最合适的回复。即 context 和 reply 的匹配。

* 训练集：由3924370个对话session组成。每个session可以看作是context+reply。
* 验证集：验证集提供10000个context和10个候选的reply，并附标准答案。
* 测试集：测试集提供10000个context和10个候选的reply。

检索评价标准是 p@1, 也可说是标准答案排在第一位的比例。

## 本次作业的目标

* 熟悉基于当前最佳预训练数据的工具。

## 前期调研

1. 一开始有pytorch-pretrained-bert和ERNIE可以用。ERNIE只有百度paddlepaddle的，学习成本太大，不用。
2. 经调查pytorch-pretrained-bert里面只有BERT支持中文，故选择BERT。
3. 看谷歌官方文档显示，只有中文的话最好选bert-base-chinese，而不是多语言版本，所以就选这个模型。
4. 观察提供的所有模型，BertModel和BertForNextSentencesPrediction可用。后者完全符合这个任务.
5. example里面run_squad.py可用。

## 数据操作

  1. BERT中文切词比较特殊，是把汉语语句切成一个字一个字的。查阅[源码](https://github.com/RayXu14/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py#L2079) 得知这些我不需要手动去做这件事，只要原始的中文句子输入即可。
  2. 对训练数据进行1：1的随机负采样。以全集作为训练集。
  3. 将"[CLS] context [SEP] reply [SEP]"交给BERT模型。长度超出512的部分取末512个token。

## 模型选择

bert-base-chinese的BertModel和BertForNextSentencesPrediction模型。

## 训练操作

* finetune整个BERT的参数。

* 采用[fp16模式](https://github.com/NVIDIA/apex)进行训练。
* BertAdam优化器，linear learning rate warmup rate=10%%。
* learning_rate = 5e-5
* weight_decay=0.01 & no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']

* num_train_epochs = 1
* train_batch_size = 5
* test_batch_size = 10

## 结果

* 学习了100,000个样本后验证集上p@1达到最高点0.5741。
* 后面loss继续降低了一段时间，但是p@1未能再上升。另外，后期loss不断升高，p@1也降到了0.1左右。估计我写了bug在里头，但暂时不明白为什么。

## 环境和训练时间

* NVIDIA RTX 2070
* NVIDIA Driver Version: 418.56
* docker image: [frontlibrary/pytorch:devel-ubuntu18.04-python3.7-cuda10.0-cudnn7-pytorch1.0.0-jupyterlab0.35.4-pretrained.bert0.6.1](https://cloud.docker.com/u/frontlibrary/repository/docker/frontlibrary/pytorch)
* 训练一个epoch的时间为35h。

## 踩坑

* NVIDIA提供的fp16模式可以节省显存，但[未必加快速度](https://mli.github.io/2016/06/14/new-pascal/ )。
* [pytorch的DataLoader多线程时有严重bug，内存占用会不断增长](https://github.com/pytorch/pytorch/issues/13246)。
